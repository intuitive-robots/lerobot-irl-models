# BESO Policy Configuration with CLIP + Language Conditioning

# Vision Encoder: Use CLIP
use_clip_encoder: true
clip_model_name: "openai/clip-vit-base-patch32"
clip_feature_dim: 512
freeze_clip: true

# Language Conditioning: Enable language instructions
use_language_conditioning: true
language_feature_dim: 512
max_language_tokens: 77

# Standard BESO parameters
sigma_data: 0.5
sigma_max: 80.0
sigma_min: 0.001
sampling_steps: 8
sampling_type: "ddim"
sigma_sample_density_type: "loglogistic"
embed_dim: 448

# Vision preprocessing
crop_shape: null
crop_is_random: false

# Diffusion parameters
horizon: 16
n_action_steps: 8
n_obs_steps: 2
num_inference_steps: null
do_mask_loss_for_padding: false

# Training parameters
use_separate_rgb_encoder_per_camera: false
