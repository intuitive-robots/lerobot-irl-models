from typing import Any, Tuple

import torch
from lerobot.configs.types import PipelineFeatureType, PolicyFeature
from lerobot.processor import (
    AddBatchDimensionProcessorStep,
    ComplementaryDataProcessorStep,
    DeviceProcessorStep,
    NormalizerProcessorStep,
    PolicyAction,
    PolicyProcessorPipeline,
    ProcessorStepRegistry,
    RenameObservationsProcessorStep,
    UnnormalizerProcessorStep,
)
from lerobot.processor.converters import (
    policy_action_to_transition,
    transition_to_policy_action,
)
from lerobot.utils.constants import (
    POLICY_POSTPROCESSOR_DEFAULT_NAME,
    POLICY_PREPROCESSOR_DEFAULT_NAME,
)


@ProcessorStepRegistry.register(name="flower_new_line_processor")
class FlowerNewLineProcessor(ComplementaryDataProcessorStep):
    """
    Ensure 'task' text ends with a newline. Helpful for some tokenizers.
    Mirrors SmolVLA's behavior.
    """

    def complementary_data(self, complementary_data):
        if "task" not in complementary_data:
            return complementary_data

        task = complementary_data["task"]
        if task is None:
            return complementary_data

        new_complementary_data = dict(complementary_data)

        if isinstance(task, str):
            if not task.endswith("\n"):
                new_complementary_data["task"] = f"{task}\n"
        elif isinstance(task, list) and all(isinstance(t, str) for t in task):
            new_complementary_data["task"] = [
                t if t.endswith("\n") else f"{t}\n" for t in task
            ]

        return new_complementary_data

    def transform_features(
        self, features: dict[PipelineFeatureType, dict[str, PolicyFeature]]
    ) -> dict[PipelineFeatureType, dict[str, PolicyFeature]]:
        return features


def make_flower_pre_post_processors(
    config: Any,
    dataset_stats: dict[str, dict[str, torch.Tensor]] | None = None,
    **kwargs,
) -> Tuple[
    PolicyProcessorPipeline[dict[str, Any], dict[str, Any]],
    PolicyProcessorPipeline[PolicyAction, PolicyAction],
]:
    """
    Build pre- and post-processor pipelines for the Flower policy.

    Notes/assumptions:
    - Tokenizer: we use the Florence-2 tokenizer via TokenizerProcessorStep with
      tokenizer_name=config.vlm_path.
    - Padding/max_length: use reasonable defaults if not configured.
    - Normalization: if input/output shape maps are missing, Normalizer steps
      degrade to no-ops.
    """

    # Derive feature maps and normalization config (gracefully handle missing fields)
    input_features = (
        getattr(config, "input_shapes", None)
        or getattr(config, "input_features", None)
        or {}
    )
    output_features = (
        getattr(config, "output_shapes", None)
        or getattr(config, "output_features", None)
        or {}
    )

    normalization_mapping = (
        getattr(config, "input_normalization_modes", None)
        or getattr(config, "normalization_mapping", None)
        or {}
    )

    input_steps = [
        RenameObservationsProcessorStep(rename_map={}),
        AddBatchDimensionProcessorStep(),
        FlowerNewLineProcessor(),
        DeviceProcessorStep(device="cuda"),
        NormalizerProcessorStep(
            features={**input_features, **output_features},
            norm_map=normalization_mapping,
            stats=dataset_stats,
        ),
    ]

    output_steps = [
        UnnormalizerProcessorStep(
            features=output_features,
            norm_map=normalization_mapping,
            stats=dataset_stats,
        ),
        DeviceProcessorStep(device="cpu"),
    ]

    return (
        PolicyProcessorPipeline[dict[str, Any], dict[str, Any]](
            steps=input_steps, name=POLICY_PREPROCESSOR_DEFAULT_NAME
        ),
        PolicyProcessorPipeline[PolicyAction, PolicyAction](
            steps=output_steps,
            name=POLICY_POSTPROCESSOR_DEFAULT_NAME,
            to_transition=policy_action_to_transition,
            to_output=transition_to_policy_action,
        ),
    )
